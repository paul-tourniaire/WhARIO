{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "significant-investing",
   "metadata": {},
   "source": [
    "# Survival analysis\n",
    "\n",
    "This notebook corresponds to step 3 of the WhARIO pipeline (cf. README.md). The notebook format allows to plot the Kaplan-Meier curves and select features or seeds based on the results of various experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-breeding",
   "metadata": {},
   "source": [
    "## Imports and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "from random import randint\n",
    "from statistics import NormalDist\n",
    "\n",
    "from h5py import File\n",
    "import lifelines\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.fitters.coxph_fitter import CoxPHFitter\n",
    "from lifelines.statistics import (\n",
    "    logrank_test, pairwise_logrank_test\n",
    ")\n",
    "from lifelines.utils import concordance_index\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import is used to ensure that the various processes\n",
    "# created by the call to the Pool() class from the\n",
    "# multiprocessing library do not freeze. If fit_model_cv\n",
    "# is defined inside the notebook, we observed the script\n",
    "# would hang randomly during execution.\n",
    "from workers import fit_model_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data, confidence=0.95):\n",
    "    dist = NormalDist.from_samples(data)\n",
    "    z = NormalDist().inv_cdf((1 + confidence) / 2.)\n",
    "    h = dist.stdev * z / ((len(data) - 1) ** .5)\n",
    "    left = round(dist.mean - h, 4)\n",
    "    right = round(dist.mean + h, 4)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    '''Load the table of clinical data.'''\n",
    "    usecols = [\n",
    "        'case_id', 'slide_id', 'PD-L1_tumor', 'recist',\n",
    "        'io_response', 'os', 'os_months', 'os_censored',\n",
    "        'center'\n",
    "    ]\n",
    "    df = pd.read_excel(f'./{filename}', usecols=usecols)\n",
    "    # Filter out patients whose survival information is missing\n",
    "    df = df.dropna(subset=['os_months'])\n",
    "    df = df[df['os_months'] > 0]\n",
    "    # Add event observation (opposite of censorship)\n",
    "    df = df.assign(event_observed=1-df['os_censored'])\n",
    "    # Filter out patients whose Tumor Proportion Score (TPS)\n",
    "    # is unknown\n",
    "    df = df.dropna(subset=['PD-L1_tumor'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_clusters(df_clini, desc_path):\n",
    "    '''Retrieve the total number of clusters'''\n",
    "    case_id_for_count = df_clini.loc[\n",
    "        df_clini.index[0], 'case_id'\n",
    "    ]\n",
    "    desc_filepath = os.path.join(\n",
    "        desc_path, f\"{case_id_for_count}.pkl\"\n",
    "    )\n",
    "    with open(desc_filepath, 'rb') as f:\n",
    "        desc = pickle.load(f)\n",
    "    return len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_descriptors(\n",
    "        df_clini, desc_path, apply_ratio=False):\n",
    "    \"\"\"\n",
    "    Load patient descriptors for each case in df_clini.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_clini: pandas.DataFrame\n",
    "        Contains the cases with clinical information.\n",
    "    desc_path: str\n",
    "        Path to the files containing patient descriptors.\n",
    "    apply_ratio: bool\n",
    "        If true, divide each row by the colomn-wise sum.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df_cl: pandas.DataFrame\n",
    "        The case-wise latent representations.\n",
    "    \"\"\"\n",
    "    case_ids = []\n",
    "    case_descs = []\n",
    "    to_remove = []\n",
    "    num_clusters = get_num_clusters(df_clini, desc_path)\n",
    "    for case_id in df_clini['case_id'].drop_duplicates():\n",
    "        desc_filepath = os.path.join(\n",
    "            desc_path, f\"{case_id}.pkl\"\n",
    "        )\n",
    "        if os.path.exists(desc_filepath):\n",
    "            case_ids.append(case_id)\n",
    "            with open(desc_filepath, 'rb') as f:\n",
    "                desc = pickle.load(f)\n",
    "            if apply_ratio:\n",
    "                desc = (\n",
    "                    desc\n",
    "                    /\n",
    "                    (np.sum(desc, axis=1).reshape(-1, 1)+1e-11)\n",
    "                )\n",
    "            case_descs.append(desc.ravel())\n",
    "        else:\n",
    "            to_remove.append(case_id)\n",
    "\n",
    "    cl_dict = dict(zip(case_ids, case_descs))\n",
    "    columns = [\n",
    "        f'h{i}-{j}'\n",
    "        for i in range(num_clusters)\n",
    "        for j in range(num_clusters+1)\n",
    "    ]\n",
    "    df_cl = pd.DataFrame.from_dict(\n",
    "        cl_dict, orient='index', columns=columns\n",
    "    )\n",
    "    df_cl.index.name = 'case_id'\n",
    "    return df_cl, to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_w_cl(df, df_cl, to_remove):\n",
    "    '''\n",
    "    Join cluster-wise latent representations with clinical\n",
    "    data.\n",
    "    '''\n",
    "    df = df.drop_duplicates(subset='case_id')\n",
    "    df = df[~df['case_id'].isin(to_remove)]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_w_cl = df.join(df_cl, on='case_id')\n",
    "    return df_w_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_km_curves(df, ax, pval=None, title=None, **km_kwargs):\n",
    "    '''Kaplan-Meier curves plotting'''\n",
    "    fitters = {\n",
    "        group: KaplanMeierFitter()\n",
    "        for group in pd.unique(df[\"risk_group\"])\n",
    "    }\n",
    "    colors = {'high': '#ff5647', 'low': '#69adff'}\n",
    "    for group in fitters.keys():\n",
    "        durations = df[df['risk_group'] == group]['os_months']\n",
    "        event_observed = (\n",
    "            df[df['risk_group'] == group]['event_observed']\n",
    "        )\n",
    "        fitters[group].fit(durations, event_observed)\n",
    "        fitters[group].plot(\n",
    "            ax=ax, label=group, color=colors[group],\n",
    "            **km_kwargs\n",
    "        )\n",
    "    ax.set_xlabel('Months after treatment start')\n",
    "    ax.set_ylabel('Probability of survival')\n",
    "\n",
    "    if pval is not None:\n",
    "        ax.text(\n",
    "            0.8, 0.75,\n",
    "            f'p = {pval:.3e}',\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "\n",
    "    if title is not None:\n",
    "        ax.set_title(f\"Stratification by median risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rank_pvalue(df):\n",
    "    event_durations = df['os_months']\n",
    "    groups = df['risk_group']\n",
    "    event_observed = df['event_observed']\n",
    "    statsresults = pairwise_logrank_test(\n",
    "        event_durations, groups, event_observed\n",
    "    )\n",
    "    return statsresults.summary.loc['high', 'p'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_risk_groups(df, quantile=0.5):\n",
    "    thresh = df['predicted_risk'].quantile(q=quantile)\n",
    "    df['risk_group'] = df['predicted_risk'].map(\n",
    "        lambda x: 'low' if x < thresh else 'high'\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "        model, test_set, *covariates,\n",
    "        optimal_risk=None, quantile=0.5):\n",
    "    '''\n",
    "    Computes C-index and log-rank test on the test-set.\n",
    "    '''\n",
    "    test_set_ = test_set.loc[\n",
    "        :,\n",
    "        list(covariates)+['os_months', 'event_observed']\n",
    "    ]\n",
    "    test_c_index = model.score(\n",
    "        test_set_,\n",
    "        scoring_method='concordance_index'\n",
    "    )\n",
    "    if isinstance(model, CoxPHFitter):\n",
    "        test_risks = model.predict_log_partial_hazard(test_set_)\n",
    "    test_set = test_set.assign(predicted_risk=test_risks)\n",
    "    if optimal_risk is None:\n",
    "        optimal_risk = (\n",
    "            test_set['predicted_risk'].quantile(q=quantile)\n",
    "        )\n",
    "    test_set['risk_group'] = test_set['predicted_risk'].map(\n",
    "        lambda x: 'low' if x < optimal_risk else 'high'\n",
    "    )\n",
    "    return test_set, test_c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_no_features(df, *features):\n",
    "    '''\n",
    "    Tests which cases in df have none of the features. Consequently,\n",
    "    the predicted risk of these cases is no more than the baseline\n",
    "    hazard.\n",
    "    '''\n",
    "    check = df[['case_id']+list(features)]\n",
    "    check = check.set_index('case_id')\n",
    "    feat_sum = check.sum(axis=1)\n",
    "    return check[feat_sum == 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-sapphire",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "clini = load_dataset(\n",
    "    './clinical_data.xlsx'\n",
    ")\n",
    "desc_path = \"./patient_descriptors/\"\n",
    "cluster_distrib_path = './cluster_distrib/'\n",
    "clini_os = filter_os_months(clini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cluster adjacency matrics and\n",
    "# derive patient descriptors\n",
    "num_clusters = get_num_clusters(clini_os, desc_path)\n",
    "apply_ratio = True\n",
    "df, to_remove = get_patient_descriptors(\n",
    "    clini_os, desc_path, apply_ratio=apply_ratio\n",
    ")\n",
    "df = join_df_w_cl(clini_os, df, to_remove)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection process: add features to the selection\n",
    "# set as long as the metric improves.\n",
    "# The metric is - c / log10(p) as explained in the paper.\n",
    "all_features = [\n",
    "    f'h{i}-{j}'\n",
    "    for i in range(num_clusters)\n",
    "    for j in range(num_clusters+1)\n",
    "]\n",
    "feature_set = all_features.copy()\n",
    "selection = []\n",
    "best_score = np.inf\n",
    "while True:\n",
    "    subset_metrics = []\n",
    "    for i, feat in enumerate(feature_set):\n",
    "        all_seeds = []\n",
    "        summaries = []\n",
    "        seed = [randint(0, 10_000) for _ in range(30)]\n",
    "        args = [\n",
    "            (df, selection+[feat], 0.5, s)\n",
    "            for s in seed\n",
    "        ]\n",
    "        with Pool() as pool:\n",
    "            res = pool.starmap(fit_model_cv, args)\n",
    "        summaries = [r[2] for r in res]\n",
    "        summaries = pd.concat(summaries, ignore_index=True)\n",
    "        summaries = summaries.drop(\n",
    "            columns=['c_index_CI', 'c_index_std']\n",
    "        )\n",
    "        summaries['ci_to_log_p_ratio'] = (\n",
    "            summaries['c_index_mean']\n",
    "            / (-np.log10(summaries['log-rank pvalue']))\n",
    "        )\n",
    "        summaries['score'] = (\n",
    "            1 / (2*summaries['c_index_mean'])\n",
    "            + summaries['ci_to_log_p_ratio']\n",
    "        )\n",
    "        mean_stats = summaries.mean(axis=0).to_frame().T\n",
    "        mean_stats.index = [feat]\n",
    "        subset_metrics.append(mean_stats)\n",
    "    subset_metrics = pd.concat(subset_metrics)\n",
    "    best = subset_metrics.sort_values(\n",
    "        by=['score']).iloc[0]\n",
    "    best_ratio = best['score']\n",
    "    best_feat = best.name\n",
    "    if best_ratio < best_score:\n",
    "        best_score = best_ratio\n",
    "        # add feature to the selected set\n",
    "        selection.append(best_feat)\n",
    "        # remove feature from the exploration space\n",
    "        feature_set.remove(best_feat)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_model_fit(df, feature_set, nseeds=30):\n",
    "    '''\n",
    "    Fits a Cox PH model on a given feature multiple times\n",
    "    to find a good seed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        The data frame containing the features\n",
    "    feature_set: list\n",
    "        The list of features to be kept in df\n",
    "    nseeds: int\n",
    "        The number of repetitions with a different seed\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    summaries: pandas.DataFrame\n",
    "        A data frame summarizing the results obtained for\n",
    "        each experiment (C index, p-values, etc...)\n",
    "    '''\n",
    "    summaries = []\n",
    "    for _ in range(30):\n",
    "        seed = randint(0, 10_000)\n",
    "        _, _, summary, _ = fit_model_cv(\n",
    "            df, feature_set, seed=seed\n",
    "        )\n",
    "        summary.index = [seed]\n",
    "        summaries.append(summary)\n",
    "\n",
    "    summaries = pd.concat(summaries)\n",
    "    summaries = summaries.drop(columns=['c_index_CI'])\n",
    "    summaries['ci_to_log_p_ratio'] = (\n",
    "        summaries['c_index_mean']\n",
    "        / (-np.log10(summaries['log-rank pvalue']))\n",
    "    )\n",
    "    summaries['score'] = (\n",
    "        1 / (2*summaries['c_index_mean'])\n",
    "        + summaries['ci_to_log_p_ratio']\n",
    "    )\n",
    "    mean_stats = summaries.mean(axis=0).to_frame().T\n",
    "    mean_stats.index = ['avg']\n",
    "    min_stats = summaries.min(axis=0).to_frame().T\n",
    "    min_stats.index = ['min']\n",
    "    max_stats = summaries.max(axis=0).to_frame().T\n",
    "    max_stats.index = ['max']\n",
    "    summaries = pd.concat(\n",
    "        (summaries, mean_stats, min_stats, max_stats)\n",
    "    )\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_curves_and_summary(df, feature_set, seed=0):\n",
    "    '''\n",
    "    Plot the Kaplan-Meier curves for a specific seed, and print the\n",
    "    metrics' summary of the model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        The data frame containing the features\n",
    "    feature_set: list\n",
    "        The list of features to be kept in df\n",
    "    nseeds: int\n",
    "        The number of repetitions with a different seed\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    df_risk: pandas.DataFrame\n",
    "        A data frame summarizing the results obtained for\n",
    "        each experiment (C index, p-values, etc...)\n",
    "    '''\n",
    "    # Plot the Kaplan-Meier curves for a specific seed\n",
    "    df_risk, results, summary, models = fit_model_cv(\n",
    "        df, feature_set, seed=seed\n",
    "    )\n",
    "    print(results.to_markdown(floatfmt=\".3f\"))\n",
    "    print()\n",
    "    summary.index = [seed]\n",
    "    summary.index.name = 'seed'\n",
    "    print(summary.to_markdown(floatfmt=\".4f\"))\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plot_km_curves(\n",
    "        df_risk, ax, pval=summary.loc[seed, 'log-rank pvalue'],\n",
    "        show_censors=True\n",
    "    )\n",
    "    # Print p-values for the features used by the model\n",
    "    cox_pvals = []\n",
    "    for i in range(5):\n",
    "        cox_pvals.append(models[i].summary['p'].to_frame())\n",
    "    print(pd.concat(cox_pvals, axis=1).to_markdown(floatfmt=\".5f\"))\n",
    "    \n",
    "    return df_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hazard_ratio(df_risk):\n",
    "    '''\n",
    "    Based on the risk groups returned by the Cox PH model,\n",
    "    compute the Hazard Ratios (HRs).\n",
    "    '''\n",
    "    for_hr_comp = df_risk[\n",
    "        ['os_months', 'event_observed', 'risk_group']\n",
    "    ].copy()\n",
    "    for_hr_comp['risk_group'] = for_hr_comp['risk_group'].map(\n",
    "        {'low': 0, 'high': 1}\n",
    "    )\n",
    "    cph_hr = CoxPHFitter()\n",
    "    cph_hr.fit(\n",
    "        for_hr_comp, duration_col='os_months',\n",
    "        event_col='event_observed'\n",
    "    )\n",
    "    print(cph_hr.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = repeat_model_fit(df, selection)\n",
    "print(summaries.to_markdown(floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-envelope",
   "metadata": {},
   "source": [
    "The previous print shows the results obtained with every `nseeds` value. One can select from the latter the best seed to plot the KM curves and print the survival metrics (C-index, p-values of the features, Hazard Ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1  # enter the chosen seed here\n",
    "km_curves_and_summary(df, selection, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-beads",
   "metadata": {},
   "source": [
    "## Keep significant coefficients\n",
    "\n",
    "At this point, from the results of the experiment above, one can remove the features for which the p-value is above the 0.05 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with p > 0.05\n",
    "feature_set = [] # enter the features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = repeat_model_fit(df, feature_set)\n",
    "print(summary.to_markdown(floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1  # enter the chosen seed here\n",
    "df_risk = km_curves_and_summary(df, feature_set, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_hazard_ratio(df_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-championship",
   "metadata": {},
   "source": [
    "## Combine features with PD-L1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-witch",
   "metadata": {},
   "source": [
    "### PD-L1 alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1  # enter the chosen seed here\n",
    "df_risk = km_curves_and_summary(df, ['PD-L1_tumor'], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_hazard_ratio(df_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-serum",
   "metadata": {},
   "source": [
    "### Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = repeat_model_fit(df, feature_set)\n",
    "print(summary.to_markdown(floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1  # enter the chosen seed here\n",
    "df_risk = km_curves_and_summary(\n",
    "    df, ['PD-L1_tumor']+feature_set, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_hazard_ratio(df_risk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
